{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from DeepER import init_embeddings_index, init_embeddings_model, init_DeepER_model, train_model_ER, replace_last_layer, model_statistics\n",
    "from experimental_similarity import mono_vector, cosine_similarity_vector, distance_similarity_vector\n",
    "from csv2dataset import splitting_dataSet, csv_2_datasetALTERNATE, csvTable2datasetRANDOM,parsing_anhai_data\n",
    "from generate_similarity_vector import generate_similarity_vector\n",
    "from data_reg import sim_hamming\n",
    "from keras.models import load_model\n",
    "from random import shuffle\n",
    "import utilist as uls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Costruzione indice degli embeddings.....Fatto. 400000 embeddings totali.\n",
      "* Creazione del modello per il calcolo degli embeddings....\n",
      "* Inizializzo il tokenizzatore.....Fatto: 400000 parole totali.\n",
      "* Preparazione della matrice di embedding.....Fatto. Dimensioni matrice embeddings: (400001, 100)\n",
      "\n",
      "°°° EMBEDDING MODEL °°°\n",
      "WARNING:tensorflow:From C:\\Users\\marte\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Tupla_A (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Tupla_B (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding_lookup (Embedding)    (None, None, 100)    40000100    Tupla_A[0][0]                    \n",
      "                                                                 Tupla_B[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 40,000,100\n",
      "Trainable params: 0\n",
      "Non-trainable params: 40,000,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Caricamento strutture dati e modelli ausiliari.\n",
    "embeddings_index = init_embeddings_index('embeddings/glove.6B.100d.txt')\n",
    "emb_dim = len(embeddings_index['cat']) # :3\n",
    "embeddings_model, tokenizer = init_embeddings_model(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_FROM_DISK_DATASET = True\n",
    "DATASET_NAME ='walmart_amazon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match_number: 1154\n",
      "len all dataset: 2308\n"
     ]
    }
   ],
   "source": [
    "# Caricamento dati e split iniziale.\n",
    "if LOAD_FROM_DISK_DATASET:\n",
    "    \n",
    "    # Carica dataset salvato su disco.\n",
    "    data = uls.load_list(f'dataset_{DATASET_NAME}')\n",
    "    match_number=sum(map(lambda x : x[3] == 1, data))\n",
    "    print(\"match_number: \" + str(match_number))\n",
    "    print(\"len all dataset: \"+ str(len(data)))\n",
    "\n",
    "else:\n",
    "    \n",
    "    GROUND_TRUTH_FILE = 'matches_fodors_zagats.csv'# Esempio: 'matches_walmart_amazon.csv'\n",
    "    # Necessario inserire le tabelle nell'ordine corrispondente alle coppie della ground truth.\n",
    "    TABLE1_FILE = 'fodors.csv'# Esempio: 'walmart.csv'\n",
    "    TABLE2_FILE = 'zagats.csv'# Esempio: 'amazon.csv'\n",
    "\n",
    "    # Coppie di attributi considerati allineati.\n",
    "    att_indexes = [(1, 1), (2, 2), (3, 3), (4, 4),(5, 5), (6, 6)]# Esempio: [(5, 9), (4, 5), (3, 3), (14, 4), (6, 11)]\n",
    "\n",
    "\n",
    "    # Crea il dataset.\n",
    "    data = csv_2_datasetALTERNATE(GROUND_TRUTH_FILE, TABLE1_FILE, TABLE2_FILE, att_indexes)\n",
    "    #per i dataset di Anhai\n",
    "    #data=parsing_anhai_data(GROUND_TRUTH_FILE, TABLE1_FILE, TABLE2_FILE, att_indexes)\n",
    "    \n",
    "    # Salva dataset su disco.\n",
    "    uls.save_list(data, f'dataset_{DATASET_NAME}')\n",
    "\n",
    "    \n",
    "# Dataset per DeepER classico: [(tupla1, tupla2, label), ...].\n",
    "deeper_data = list(map(lambda q: (q[0], q[1], q[3]), data))\n",
    "\n",
    "\n",
    "# Taglia attributi se troppo lunghi\n",
    "# Alcuni dataset hanno attributi con descrizioni molto lunghe.\n",
    "# Questo filtro limita il numero di caratteri di un attributo a 1000.\n",
    "def shrink_data(data):\n",
    "    \n",
    "    def cut_string(s):\n",
    "        if len(s) >= 1000:\n",
    "            return s[:1000]\n",
    "        else:\n",
    "            return s\n",
    "    \n",
    "    temp = []\n",
    "    for t1, t2, lb in data:\n",
    "        t1 = list(map(cut_string, t1))\n",
    "        t2 = list(map(cut_string, t2))\n",
    "        temp.append((t1, t2, lb))\n",
    "        \n",
    "    return temp\n",
    "\n",
    "deeper_data = shrink_data(deeper_data)\n",
    "\n",
    "\n",
    "# Split in training set e test set.\n",
    "def split_training_test(data, SPLIT_FACTOR = 0.8):     \n",
    "    bound = int(len(data) * SPLIT_FACTOR)\n",
    "    train = data[:bound]\n",
    "    test = data[bound:]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "# Tutti i successivi addestramenti partiranno dal 100% di deeper_train (80% di tutti i dati).\n",
    "# Le tuple in deeper_test non verranno mai usate per addestrare ma solo per testare i modelli.\n",
    "deeper_train, deeper_test = split_training_test(deeper_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "\n",
    "def get_layer_output(model,layer_name,data):\n",
    "    intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "    intermediate_output = intermediate_layer_model.predict(data)\n",
    "    return intermediate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_output_grad(model, inputs, outputs, layer_name):\n",
    "    \"\"\" Gets gradient a layer output for given inputs and outputs\"\"\"\n",
    "    grads = model.optimizer.get_gradients(model.total_loss, model.get_layer(layer_name).output)\n",
    "    symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
    "    f = K.function(symb_inputs, grads)\n",
    "    x, y, sample_weight = model._standardize_user_data(inputs, outputs)\n",
    "    output_grad = f(x + y + sample_weight)\n",
    "    return output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_submodel_fromModel(model,layer_idx,input_shape):\n",
    "    layer_input = Input(shape= input_shape) # a new input tensor to be able to feed the desired layer\n",
    "    # create the new nodes for each layer in the path\n",
    "    x = layer_input\n",
    "    for layer in walmart_amazon_model.layers[layer_idx:]:\n",
    "        x = layer(x)\n",
    "    # create the model\n",
    "    new_model = Model(layer_input, x)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and extract layer input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNegative(sample):\n",
    "    return sample[2]==0\n",
    "train_negatives = list(filter(isNegative,deeper_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1846, 923)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deeper_train),len(train_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Preparazione input......Fatto. 1846 tuple totali, esempio label: 1 -> 1, Table1 shape: (1846, 33), Table2 shape: (1846, 13)\n",
      "WARNING:tensorflow:From C:\\Users\\marte\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "* Preparazione input......Fatto. 923 tuple totali, esempio label: 0 -> 0, Table1 shape: (923, 33), Table2 shape: (923, 12)\n"
     ]
    }
   ],
   "source": [
    "from DeepER import data2Inputs\n",
    "table1, table2, labels = data2Inputs(deeper_train, tokenizer, categorical=False)\n",
    "embeddings = embeddings_model.predict([table1,table2])\n",
    "table1_neg,table2_neg, labels = data2Inputs(train_negatives,tokenizer,categorical=False)\n",
    "negEmbeddings = embeddings_model.predict([table1_neg,table2_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Embeddings_seq_a (InputLayer)   (None, None, 100)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embeddings_seq_b (InputLayer)   (None, None, 100)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Composition (Bidirectional)     (None, 300)          301200      Embeddings_seq_a[0][0]           \n",
      "                                                                 Embeddings_seq_b[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Similarity (Lambda)             (None, 300)          0           Composition[0][0]                \n",
      "                                                                 Composition[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Dense1 (Dense)                  (None, 300)          90300       Similarity[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Dense2 (Dense)                  (None, 300)          90300       Dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Classification (Dense)          (None, 2)            602         Dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 482,402\n",
      "Trainable params: 482,402\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "walmart_amazon_model = load_model('models/DeepER_best_model_100_walmart_amazon.h5')\n",
    "walmart_amazon_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "negPredictions = walmart_amazon_model.predict(negEmbeddings)\n",
    "similarity_output_neg = get_layer_output(walmart_amazon_model,'Similarity',negEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_weights = []\n",
    "for layer in walmart_amazon_model.layers[-3:]:\n",
    "    selected_weights.append(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1_output = get_layer_output(walmart_amazon_model,'Dense1',embeddings)\n",
    "dense2_output = get_layer_output(walmart_amazon_model,'Dense2',embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classication_output = softmax(getActivation(classification_weights[1],classification_weights[0],dense2_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_out_real = get_layer_output(walmart_amazon_model,'Classification',embeddings)\n",
    "classification_out_real[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "outputTensor = walmart_amazon_model.output\n",
    "trainableWeights = walmart_amazon_model.trainable_weights\n",
    "gradients = K.gradients(outputTensor,trainableWeights)\n",
    "l_exampleInput = embeddings[0][0:8]\n",
    "r_exampleInput = embeddings[1][0:8]\n",
    "\n",
    "with tf.Session() as s:\n",
    "    s.run(tf.global_variables_initializer())\n",
    "    evaluated_gradients = s.run(gradients,feed_dict={walmart_amazon_model.input[0]:l_exampleInput,\n",
    "                                                    walmart_amazon_model.input[1]:r_exampleInput})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_output = walmart_amazon_model.layers[-4].output\n",
    "model_output = walmart_amazon_model.output\n",
    "custom_loss = K.max(model_output,axis=1)\n",
    "gradients = K.gradients(custom_loss,walmart_amazon_model.get_layer('Dense1').input)\n",
    "with tf.Session() as s:\n",
    "    s.run(tf.global_variables_initializer())\n",
    "    evaluated_gradients = s.run(gradients,feed_dict={walmart_amazon_model.input[0]:l_exampleInput,\n",
    "                                                    walmart_amazon_model.input[1]:r_exampleInput})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30017543\n",
      "0.29355785\n",
      "0.27970394\n",
      "0.26048666\n",
      "0.28302205\n",
      "0.2617454\n",
      "0.25232735\n",
      "0.23955864\n"
     ]
    }
   ],
   "source": [
    "for grad in evaluated_gradients[0]:\n",
    "    print (np.linalg.norm(grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.nn import relu,softmax\n",
    "from tensorflow.linalg import matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createComputationGraphForGradient(weight_list,activation_f,input_batch):\n",
    "    input_t = tf.constant(input_batch)\n",
    "    layer_output = [input_t]\n",
    "    for i in range(len(weight_list)):\n",
    "        current_layer_weight = tf.constant(weight_list[i][0])\n",
    "        current_layer_bias = tf.constant(weight_list[i][1])\n",
    "        current_output = activation_f[i](matmul(layer_output[i],current_layer_weight)+current_layer_bias)\n",
    "        layer_output.append(current_output)\n",
    "    final_output_t = layer_output[-1]\n",
    "    return (input_t,final_output_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=tf.ConfigProto(log_device_placement=True)\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [relu,relu,softmax]\n",
    "initial_gradients = np.zeros((64,2)).astype(np.float32)\n",
    "initial_gradients[:,1]=1\n",
    "initial_gradients_t = tf.constant(initial_gradients)\n",
    "x,y = createComputationGraphForGradient(selected_weights,activations,similarity_output_neg[0:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.constant(initial_gradients)\n",
    "gradients = tf.gradients(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients0_15 = session.run(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grad in gradients0_15[0]:\n",
    "    print(np.linalg.norm(grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Ri algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def find_smallest_variation_to_change(weight_list,activations,input_batch,sample_idx,classifier_length,class_to_reach,\n",
    "                                     tf_session):\n",
    "    xi = input_batch[sample_idx]\n",
    "    input_batch_copy = input_batch.copy()\n",
    "    input_t,output_t = createComputationGraphForGradient(weight_list,activations,input_batch_copy)\n",
    "    current_probabilities = tf_session.run(output_t)[sample_idx]\n",
    "    sum_ri = np.zeros(classifier_length)\n",
    "    iterations = 0\n",
    "    while(round(current_probabilities[1])!=class_to_reach and iterations<50):\n",
    "        print(\"current iteration is {}\".format(str(iterations)))\n",
    "        input_t,output_t = createComputationGraphForGradient(weight_list,activations,input_batch_copy)\n",
    "        current_probabilities = tf_session.run(output_t)[sample_idx]\n",
    "        if class_to_reach == 1:\n",
    "            fx = current_probabilities[0]\n",
    "        else:\n",
    "            fx = current_probabilities[1]\n",
    "        current_gradient = tf_session.run(tf.gradients(output_t,input_t))[0][sample_idx]\n",
    "        current_norm = norm(current_gradient)\n",
    "        if current_norm==0:\n",
    "            sum_ri = np.zeros(classifier_length)\n",
    "            print(\"Gradient is null\")\n",
    "            break\n",
    "        ri = (fx/(current_norm**2)) * (-current_gradient)\n",
    "        xi = xi+ri\n",
    "        input_batch_copy[sample_idx] = xi\n",
    "        \n",
    "        sum_ri += ri\n",
    "        iterations+=1\n",
    "    if iterations>=50:\n",
    "        sum_ri = np.zeros(classifier_length)\n",
    "        print(\"can't converge \")\n",
    "        \n",
    "    return sum_ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = similarity_output_neg\n",
    "sample_idx = 0\n",
    "classifier_length = 300\n",
    "class_to_reach =0\n",
    "tf_session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "ris = []\n",
    "for i in tqdm(range(len(input_batch))):\n",
    "    ri = find_smallest_variation_to_change(selected_weights,activations,input_batch,i,classifier_length=300,class_to_reach=1,\n",
    "                                      tf_session=tf_session)\n",
    "    ris.append(ri)\n",
    "tf_session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
