{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from deeper.data import data2Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = 'itunes_amazon'\n",
    "train_df = pd.read_csv(os.path.join(DATASET_DIR,'train.csv'))\n",
    "validation_df = pd.read_csv(os.path.join(DATASET_DIR,'valid.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATASET_DIR,'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ltable_id</th>\n",
       "      <th>rtable_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1409</td>\n",
       "      <td>51560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1289</td>\n",
       "      <td>34035</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2666</td>\n",
       "      <td>35475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1157</td>\n",
       "      <td>27989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3332</td>\n",
       "      <td>7412</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ltable_id  rtable_id  label\n",
       "0       1409      51560      0\n",
       "1       1289      34035      0\n",
       "2       2666      35475      1\n",
       "3       1157      27989      0\n",
       "4       3332       7412      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.concat([train_df,validation_df,test_df])\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_df = all_df[all_df.label==1]\n",
    "positives_df = positives_df.drop(['label'], axis=1)\n",
    "positives_df.columns = ['id1','id2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_df.to_csv('itunes_amazon/matches_itunes_amazon.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function for Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_tolist(dataset,lprefix,rprefix):\n",
    "    res = []\n",
    "    attributes = [col for col in dataset.columns if col not in ['label','id']]\n",
    "    for i in range(len(dataset.index)):\n",
    "        row = dataset.iloc[i]\n",
    "        ltuple,rtuple = ([],[])\n",
    "        for attr in attributes:\n",
    "            if attr.startswith(lprefix):\n",
    "                ltuple = ltuple+ row[attr].split()\n",
    "            else:\n",
    "                rtuple = rtuple+row[attr].split()\n",
    "        res.append((ltuple,rtuple,row['label']))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_deeper(dataset,lprefix,rprefix,model,tokenizer,embedding_model):\n",
    "    dataset_list = dataset_tolist(dataset,lprefix,rprefix)\n",
    "    ltokens,rtokens,_ = data2Inputs(dataset_list,tokenizer)\n",
    "    lembeddings,rembeddings = embedding_model.predict([ltokens,rtokens])\n",
    "    predictions = model.predict([lembeddings,rembeddings])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from deeper.DeepER import init_embeddings_index, init_embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Costruzione indice degli embeddings.....Fatto. 400000 embeddings totali.\n",
      "* Creazione del modello per il calcolo degli embeddings....\n",
      "* Inizializzo il tokenizzatore.....Fatto: 400000 parole totali.\n",
      "* Preparazione della matrice di embedding.....Fatto. Dimensioni matrice embeddings: (400001, 300)\n",
      "\n",
      "°°° EMBEDDING MODEL °°°\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Tupla_A (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Tupla_B (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding_lookup (Embedding)    (None, None, 300)    120000300   Tupla_A[0][0]                    \n",
      "                                                                 Tupla_B[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 120,000,300\n",
      "Trainable params: 0\n",
      "Non-trainable params: 120,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILEPATH ='embeddings\\glove.6B\\glove.6B.300d.txt'\n",
    "# Caricamento strutture dati e modelli ausiliari.\n",
    "embeddings_index = init_embeddings_index(EMBEDDING_FILEPATH)\n",
    "emb_dim = len(embeddings_index['cat']) # :3\n",
    "embeddings_model, tokenizer = init_embeddings_model(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_model = load_model('models/DeepER_best_model_100_walmart-amazon.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Preparazione input......Fatto. 2049 tuple totali, esempio label: 0 -> [1. 0.], Table1 shape: (2049, 33), Table2 shape: (2049, 32)\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('test-walmart-amazon.csv',dtype=str).fillna(\"\")\n",
    "predictions = wrap_deeper(test_df,'ltable_','rtable_',walmart_model,tokenizer,embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18110621634850713"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "predicted_labels = np.argmax(predictions,axis=1)\n",
    "true_labels = list(map(lambda s:int(s),test_df.label.values))\n",
    "f1_score(true_labels,predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '0', '0', ..., '0', '0', '0'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
