{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepER import init_embeddings_index, init_embeddings_model, init_DeepER_model, train_model_ER, replace_last_layer, model_statistics\n",
    "from experimental_similarity import mono_vector, cosine_similarity_vector, distance_similarity_vector\n",
    "from csv2dataset import splitting_dataSet, csv_2_datasetALTERNATE, csvTable2datasetRANDOM,parsing_anhai_data\n",
    "from generate_similarity_vector import generate_similarity_vector\n",
    "from data_reg import sim_hamming\n",
    "from keras.models import load_model\n",
    "from random import shuffle\n",
    "import utilist as uls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento strutture dati e modelli ausiliari.\n",
    "embeddings_index = init_embeddings_index('embeddings/glove.6B.100d.txt')\n",
    "emb_dim = len(embeddings_index['cat']) # :3\n",
    "embeddings_model, tokenizer = init_embeddings_model(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento dati e split iniziale.\n",
    "if LOAD_FROM_DISK_DATASET:\n",
    "    \n",
    "    # Carica dataset salvato su disco.\n",
    "    data = uls.load_list(f'dataset_{DATASET_NAME}')\n",
    "    match_number=sum(map(lambda x : x[3] == 1, data))\n",
    "    print(\"match_number: \" + str(match_number))\n",
    "    print(\"len all dataset: \"+ str(len(data)))\n",
    "\n",
    "else:\n",
    "    \n",
    "    GROUND_TRUTH_FILE = 'matches_fodors_zagats.csv'# Esempio: 'matches_walmart_amazon.csv'\n",
    "    # Necessario inserire le tabelle nell'ordine corrispondente alle coppie della ground truth.\n",
    "    TABLE1_FILE = 'fodors.csv'# Esempio: 'walmart.csv'\n",
    "    TABLE2_FILE = 'zagats.csv'# Esempio: 'amazon.csv'\n",
    "\n",
    "    # Coppie di attributi considerati allineati.\n",
    "    att_indexes = [(1, 1), (2, 2), (3, 3), (4, 4),(5, 5), (6, 6)]# Esempio: [(5, 9), (4, 5), (3, 3), (14, 4), (6, 11)]\n",
    "\n",
    "\n",
    "    # Crea il dataset.\n",
    "    data = csv_2_datasetALTERNATE(GROUND_TRUTH_FILE, TABLE1_FILE, TABLE2_FILE, att_indexes)\n",
    "    #per i dataset di Anhai\n",
    "    #data=parsing_anhai_data(GROUND_TRUTH_FILE, TABLE1_FILE, TABLE2_FILE, att_indexes)\n",
    "    \n",
    "    # Salva dataset su disco.\n",
    "    uls.save_list(data, f'dataset_{DATASET_NAME}')\n",
    "\n",
    "    \n",
    "# Dataset per DeepER classico: [(tupla1, tupla2, label), ...].\n",
    "deeper_data = list(map(lambda q: (q[0], q[1], q[3]), data))\n",
    "\n",
    "\n",
    "# Taglia attributi se troppo lunghi\n",
    "# Alcuni dataset hanno attributi con descrizioni molto lunghe.\n",
    "# Questo filtro limita il numero di caratteri di un attributo a 1000.\n",
    "def shrink_data(data):\n",
    "    \n",
    "    def cut_string(s):\n",
    "        if len(s) >= 1000:\n",
    "            return s[:1000]\n",
    "        else:\n",
    "            return s\n",
    "    \n",
    "    temp = []\n",
    "    for t1, t2, lb in data:\n",
    "        t1 = list(map(cut_string, t1))\n",
    "        t2 = list(map(cut_string, t2))\n",
    "        temp.append((t1, t2, lb))\n",
    "        \n",
    "    return temp\n",
    "\n",
    "deeper_data = shrink_data(deeper_data)\n",
    "\n",
    "\n",
    "# Split in training set e test set.\n",
    "def split_training_test(data, SPLIT_FACTOR = 0.8):     \n",
    "    bound = int(len(data) * SPLIT_FACTOR)\n",
    "    train = data[:bound]\n",
    "    test = data[bound:]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "# Tutti i successivi addestramenti partiranno dal 100% di deeper_train (80% di tutti i dati).\n",
    "# Le tuple in deeper_test non verranno mai usate per addestrare ma solo per testare i modelli.\n",
    "deeper_train, deeper_test = split_training_test(deeper_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepER import data2Inputs\n",
    "table1, table2, labels = data2Inputs(deeper_train, tokenizer, categorical=False)\n",
    "embeddings = embeddings_model.predict([table1,table2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = [embeddings[0][0:8],embeddings[1][0:8]]\n",
    "from keras.models import Model\n",
    "layer_name = 'Similarity'\n",
    "intermediate_layer_model = Model(inputs=deeper_model_100.input,\n",
    "                                 outputs=deeper_model_100.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(test_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
