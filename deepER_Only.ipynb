{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepER Classic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Caricamento dati, preprocessing e strutture ausiliarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from deeper.DeepER import init_embeddings_index, init_embeddings_model, init_DeepER_model, train_model_ER, model_statistics\n",
    "from deeper.data import process_data\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from deeper.csv2dataset import splitting_dataSet\n",
    "from plotly import graph_objs as go\n",
    "import plotly.offline as pyo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Costruzione indice degli embeddings.....Fatto. 400000 embeddings totali.\n",
      "* Creazione del modello per il calcolo degli embeddings....\n",
      "* Inizializzo il tokenizzatore.....Fatto: 400000 parole totali.\n",
      "* Preparazione della matrice di embedding.....Fatto. Dimensioni matrice embeddings: (400001, 300)\n",
      "\n",
      "°°° EMBEDDING MODEL °°°\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Tupla_A (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Tupla_B (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding_lookup (Embedding)    (None, None, 300)    120000300   Tupla_A[0][0]                    \n",
      "                                                                 Tupla_B[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 120,000,300\n",
      "Trainable params: 0\n",
      "Non-trainable params: 120,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Caricamento strutture dati e modelli ausiliari.\n",
    "EMBEDDING_FILEPATH ='embeddings\\glove.6B\\glove.6B.300d.txt'\n",
    "embeddings_index = init_embeddings_index(EMBEDDING_FILEPATH)\n",
    "emb_dim = len(embeddings_index['cat']) # :3\n",
    "embeddings_model, tokenizer = init_embeddings_model(embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walmart-Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imposta manualmente a False per ricreare il file contenente il dataset scelto. \n",
    "LOAD_FROM_DISK_DATASET=False\n",
    "# Imposta manualmente a False per ri-eseguire tutti gli addestramenti.\n",
    "LOAD_FROM_DISK_MODEL = False\n",
    "# Il nome con cui saranno etichettati i files prodotti\n",
    "DATASET_DIR = 'datasets/walmart_amazon/'\n",
    "DATASET_NAME ='walmart-amazon'\n",
    "TABLE1_FILE = 'walmart.csv'\n",
    "TABLE2_FILE = 'amazon.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeper_train,deeper_test = process_data(DATASET_DIR,DATASET_NAME,ground_truth='walmart-amazon_perfectMapping.csv',\n",
    "                         table1=TABLE1_FILE,table2=TABLE2_FILE,indici=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim =300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Addestramento standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# InPut: Percentuale di dati considerata per l'addestramento. \n",
    "# OutPut: DeepER addestrato sul taglio scelto.\n",
    "def get_DeepER(perc,train,LOAD_FROM_DISK_MODEL):\n",
    "   \n",
    "    sub_data = splitting_dataSet(perc,train)    \n",
    "    \n",
    "    if LOAD_FROM_DISK_MODEL:\n",
    "        \n",
    "        # Carica da disco.\n",
    "        print(f'Loading DeepER_best_model_{int(perc*100)}_{DATASET_NAME}.h5', end='', flush=True)\n",
    "        deeper_model = load_model(f'models/DeepER_best_model_{int(perc*100)}_{DATASET_NAME}.h5')\n",
    "        print('  ->  Done')        \n",
    "                \n",
    "    else:\n",
    "        \n",
    "        # Inizializza il modello.\n",
    "        deeper_model = init_DeepER_model(emb_dim)\n",
    "        deeper_model.summary()\n",
    "        # Avvio addestramento.\n",
    "        deeper_model = train_model_ER(sub_data, \n",
    "                                      deeper_model, \n",
    "                                      embeddings_model, \n",
    "                                      tokenizer, \n",
    "                                      pretraining=False,\n",
    "                                      metric='val_accuracy',\n",
    "                                      end=f'_{int(perc*100)}_{DATASET_NAME}')\n",
    "        \n",
    "    return deeper_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "°°° DeepER Model °°°\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Embeddings_seq_a (InputLayer)   (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embeddings_seq_b (InputLayer)   (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Composition (Bidirectional)     (None, 300)          541200      Embeddings_seq_a[0][0]           \n",
      "                                                                 Embeddings_seq_b[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Similarity (Lambda)             (None, 300)          0           Composition[0][0]                \n",
      "                                                                 Composition[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Dense1 (Dense)                  (None, 300)          90300       Similarity[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Dense2 (Dense)                  (None, 300)          90300       Dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Classification (Dense)          (None, 2)            602         Dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 722,402\n",
      "Trainable params: 722,402\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "* Preparazione input......Fatto. 1846 tuple totali, esempio label: 1 -> [0. 1.], Table1 shape: (1846, 38), Table2 shape: (1846, 51)\n",
      "Batch size: 29\n",
      "Train on 1476 samples, validate on 370 samples\n",
      "Epoch 1/64\n",
      "1476/1476 [==============================] - 10s 7ms/step - loss: 0.3172 - accuracy: 0.8428 - val_loss: 0.0967 - val_accuracy: 0.9649\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.96486, saving model to models\\DeepER_best_model_100_walmart-amazon.h5\n",
      "Epoch 2/64\n",
      "1476/1476 [==============================] - 9s 6ms/step - loss: 0.0520 - accuracy: 0.9851 - val_loss: 0.0651 - val_accuracy: 0.9811\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.96486 to 0.98108, saving model to models\\DeepER_best_model_100_walmart-amazon.h5\n",
      "Epoch 3/64\n",
      "1476/1476 [==============================] - 9s 6ms/step - loss: 0.0280 - accuracy: 0.9925 - val_loss: 0.0843 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.98108\n",
      "Epoch 4/64\n",
      "1476/1476 [==============================] - 9s 6ms/step - loss: 0.0134 - accuracy: 0.9959 - val_loss: 0.0893 - val_accuracy: 0.9676\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.98108\n",
      "Epoch 5/64\n",
      "1476/1476 [==============================] - 9s 6ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.0826 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.98108\n",
      "Epoch 6/64\n",
      "1476/1476 [==============================] - 9s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0721 - val_accuracy: 0.9811\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.98108\n",
      "Epoch 7/64\n",
      "1476/1476 [==============================] - 9s 6ms/step - loss: 1.3454e-04 - accuracy: 1.0000 - val_loss: 0.0675 - val_accuracy: 0.9838\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.98108 to 0.98378, saving model to models\\DeepER_best_model_100_walmart-amazon.h5\n",
      "Epoch 8/64\n",
      "1476/1476 [==============================] - 9s 6ms/step - loss: 8.2531e-05 - accuracy: 1.0000 - val_loss: 0.0668 - val_accuracy: 0.9838\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.98378\n",
      "Epoch 9/64\n",
      "1476/1476 [==============================] - 10s 7ms/step - loss: 6.2253e-05 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 0.9838\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.98378\n",
      "Epoch 10/64\n",
      "1476/1476 [==============================] - 9s 6ms/step - loss: 4.9590e-05 - accuracy: 1.0000 - val_loss: 0.0663 - val_accuracy: 0.9838\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.98378\n",
      "Epoch 11/64\n",
      "1476/1476 [==============================] - 10s 7ms/step - loss: 4.0426e-05 - accuracy: 1.0000 - val_loss: 0.0663 - val_accuracy: 0.9811\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.98378\n",
      "Epoch 12/64\n",
      "1476/1476 [==============================] - 10s 7ms/step - loss: 3.3709e-05 - accuracy: 1.0000 - val_loss: 0.0666 - val_accuracy: 0.9811\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.98378\n",
      "Epoch 13/64\n",
      "1476/1476 [==============================] - 10s 7ms/step - loss: 2.8560e-05 - accuracy: 1.0000 - val_loss: 0.0668 - val_accuracy: 0.9811\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.98378\n",
      "Epoch 14/64\n",
      "1476/1476 [==============================] - 9s 6ms/step - loss: 2.4663e-05 - accuracy: 1.0000 - val_loss: 0.0671 - val_accuracy: 0.9811\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.98378\n",
      "Epoch 00014: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Avvio addestramenti o carica da disco.\n",
    "deeper_model_100 = get_DeepER(1,deeper_train,LOAD_FROM_DISK_MODEL=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcolo F-Measure dopo addestramento standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Avvio test metriche....\n",
      "-- Corpus size: 462\n",
      "-- Non Match: 237\n",
      "-- Match: 225\n",
      "* Preparazione input......Fatto. 462 tuple totali, esempio label: 0 -> [1. 0.], Table1 shape: (462, 33), Table2 shape: (462, 49)\n",
      "* Evaluating: ===========|\n",
      "Precision: 0.9817351598173516, Recall: 0.9555555555555556, f1-score: 0.9684684684684685\n",
      "Total retrieved: 219, retrieved/total matches: 215/225\n",
      "0.9684684684684685\n"
     ]
    }
   ],
   "source": [
    "# Misurazione dell'f-measure sullo stesso test set con i diversi modelli.\n",
    "f1_score= model_statistics(deeper_test, deeper_model_100, embeddings_model, tokenizer)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizzazione F-Measure: primi risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Attiva modalità notebook per mostrare i grafici correttamente.\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "splits = ['100% split', '75% split', '50% split', '25% split', '10% split', '5% split']\n",
    "total_tup = len(deeper_train)\n",
    "tuplecount = [total_tup, \n",
    "              int(total_tup*0.75), \n",
    "              int(total_tup*0.5), \n",
    "              int(total_tup*0.25), \n",
    "              int(total_tup*0.1), \n",
    "              int(total_tup*0.05)]\n",
    "\n",
    "# Aggiungi descrizione al numero\n",
    "tuplecount = list(map(lambda x: f'{x} coppie di tuple', tuplecount))\n",
    "\n",
    "fig = go.Figure(data=[go.Bar(name='DeepER', x=splits, y=fm_model_standard, hovertext=tuplecount)])\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "# Plotta il grafico e salvalo come features_standard.html (verrà integrato nell'html).\n",
    "pyo.iplot(fig, filename='fmeasures-standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Al passaggio del mouse il grafico mostra:\n",
    "- Il numero di coppie di tuple utilizzate per l'addestramento; \n",
    "- La percentuale di split (Quantità di tuple utilizzate per addestrare il modello);\n",
    "- Il valore di F-Measure (media armonica tra precision e recall);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iTunes-Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITUNES_DIR = 'datasets/itunes_amazon/'\n",
    "DATASET_NAME ='itunes-amazon'\n",
    "TABLE1_FILE = 'itunes.csv'\n",
    "TABLE2_FILE = 'amazon.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la coseno similarity minima is 0.3446561747421316\n"
     ]
    }
   ],
   "source": [
    "itunes_train,itunes_test = process_data(ITUNES_DIR,DATASET_NAME,ground_truth='matches_itunes_amazon.csv',\n",
    "                         table1=TABLE1_FILE,table2=TABLE2_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "°°° DeepER Model °°°\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Embeddings_seq_a (InputLayer)   (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embeddings_seq_b (InputLayer)   (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Composition (Bidirectional)     (None, 300)          541200      Embeddings_seq_a[0][0]           \n",
      "                                                                 Embeddings_seq_b[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Similarity (Lambda)             (None, 300)          0           Composition[0][0]                \n",
      "                                                                 Composition[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Dense1 (Dense)                  (None, 300)          90300       Similarity[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Dense2 (Dense)                  (None, 300)          90300       Dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Classification (Dense)          (None, 2)            602         Dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 722,402\n",
      "Trainable params: 722,402\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "* Preparazione input......Fatto. 211 tuple totali, esempio label: 1 -> [0. 1.], Table1 shape: (211, 75), Table2 shape: (211, 79)\n",
      "Batch size: 4\n",
      "Train on 168 samples, validate on 43 samples\n",
      "Epoch 1/64\n",
      "168/168 [==============================] - 13s 77ms/step - loss: 0.5867 - accuracy: 0.7321 - val_loss: 0.4308 - val_accuracy: 0.7907\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.79070, saving model to models\\DeepER_best_model_100_itunes-amazon.h5\n",
      "Epoch 2/64\n",
      "168/168 [==============================] - 12s 69ms/step - loss: 0.1820 - accuracy: 0.9167 - val_loss: 0.2818 - val_accuracy: 0.8837\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.79070 to 0.88372, saving model to models\\DeepER_best_model_100_itunes-amazon.h5\n",
      "Epoch 3/64\n",
      "168/168 [==============================] - 12s 69ms/step - loss: 0.0631 - accuracy: 0.9762 - val_loss: 0.6643 - val_accuracy: 0.7674\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.88372\n",
      "Epoch 4/64\n",
      "168/168 [==============================] - 12s 70ms/step - loss: 0.0338 - accuracy: 0.9881 - val_loss: 0.1900 - val_accuracy: 0.9070\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.88372 to 0.90698, saving model to models\\DeepER_best_model_100_itunes-amazon.h5\n",
      "Epoch 5/64\n",
      "168/168 [==============================] - 12s 69ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.3896 - val_accuracy: 0.8372\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.90698\n",
      "Epoch 6/64\n",
      "168/168 [==============================] - 12s 70ms/step - loss: 0.0590 - accuracy: 0.9881 - val_loss: 0.8547 - val_accuracy: 0.7907\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.90698\n",
      "Epoch 7/64\n",
      "168/168 [==============================] - 12s 69ms/step - loss: 0.0605 - accuracy: 0.9821 - val_loss: 0.6040 - val_accuracy: 0.7674\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.90698\n",
      "Epoch 8/64\n",
      "168/168 [==============================] - 11s 68ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5021 - val_accuracy: 0.8140\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.90698\n",
      "Epoch 9/64\n",
      "168/168 [==============================] - 11s 68ms/step - loss: 7.3609e-04 - accuracy: 1.0000 - val_loss: 0.5768 - val_accuracy: 0.8140\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.90698\n",
      "Epoch 10/64\n",
      "168/168 [==============================] - 12s 69ms/step - loss: 3.9383e-04 - accuracy: 1.0000 - val_loss: 0.6221 - val_accuracy: 0.7907\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.90698\n",
      "Epoch 11/64\n",
      "168/168 [==============================] - 12s 69ms/step - loss: 2.6468e-04 - accuracy: 1.0000 - val_loss: 0.6641 - val_accuracy: 0.7907\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.90698\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Avvio addestramenti o carica da disco.\n",
    "deeper_model_itunes_100 = get_DeepER(1,itunes_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon-Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'datasets/Amazon-GoogleProducts/'\n",
    "DATASET_NAME ='amazon-google'\n",
    "TABLE1_FILE = 'Amazon.csv'\n",
    "TABLE2_FILE = 'Google.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = process_data(DATA_DIR,DATASET_NAME,ground_truth='amazon_google_matches.csv',\n",
    "                         table1=TABLE1_FILE,table2=TABLE2_FILE,indici=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "°°° DeepER Model °°°\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Embeddings_seq_a (InputLayer)   (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embeddings_seq_b (InputLayer)   (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Composition (Bidirectional)     (None, 300)          541200      Embeddings_seq_a[0][0]           \n",
      "                                                                 Embeddings_seq_b[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Similarity (Lambda)             (None, 300)          0           Composition[0][0]                \n",
      "                                                                 Composition[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Dense1 (Dense)                  (None, 300)          90300       Similarity[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Dense2 (Dense)                  (None, 300)          90300       Dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Classification (Dense)          (None, 2)            602         Dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 722,402\n",
      "Trainable params: 722,402\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "* Preparazione input......Fatto. 2080 tuple totali, esempio label: 1 -> [0. 1.], Table1 shape: (2080, 2685), Table2 shape: (2080, 73)\n",
      "Batch size: 32\n",
      "Train on 1664 samples, validate on 416 samples\n",
      "Epoch 1/64\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[32,600] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node Composition_6/while/body/_1/MatMul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_26305]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-540304383ad6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Avvio addestramenti o carica da disco.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdeeper_model_amazongoogle_100\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_DeepER\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLOAD_FROM_DISK_MODEL\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-18d0f2e00aa9>\u001b[0m in \u001b[0;36mget_DeepER\u001b[1;34m(perc, train, LOAD_FROM_DISK_MODEL)\u001b[0m\n\u001b[0;32m     24\u001b[0m                                       \u001b[0mpretraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                                       \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                                       end=f'_{int(perc*100)}_{DATASET_NAME}')\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeeper_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\DeepER\\deeper\\DeepER.py\u001b[0m in \u001b[0;36mtrain_model_ER\u001b[1;34m(data, model, embeddings_model, tokenizer, best_save_path, pretraining, metric, end)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[0mparam_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.015\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Batch size:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;31m# Carica il miglior modello checkpointed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marte\\anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\marte\\anaconda3\\envs\\mlenv\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marte\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marte\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marte\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marte\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\marte\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\marte\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mc:\\users\\marte\\anaconda3\\envs\\mlenv\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[32,600] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node Composition_6/while/body/_1/MatMul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_26305]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "# Avvio addestramenti o carica da disco.\n",
    "deeper_model_amazongoogle_100 = get_DeepER(1,train,LOAD_FROM_DISK_MODEL=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
