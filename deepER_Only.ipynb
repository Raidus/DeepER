{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepER Classic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Caricamento dati, preprocessing e strutture ausiliarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from deeper.DeepER import init_embeddings_index, init_embeddings_model, init_DeepER_model, train_model_ER, replace_last_layer, model_statistics\n",
    "from deeper.data import process_data\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from deeper.csv2dataset import splitting_dataSet\n",
    "from plotly import graph_objs as go\n",
    "import plotly.offline as pyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imposta manualmente a False per ricreare il file contenente il dataset scelto. \n",
    "LOAD_FROM_DISK_DATASET=False\n",
    "# Imposta manualmente a False per ri-eseguire tutti gli addestramenti.\n",
    "LOAD_FROM_DISK_MODEL = False\n",
    "EMBEDDING_FILEPATH ='embeddings\\glove.6B\\glove.6B.300d.txt'\n",
    "# Il nome con cui saranno etichettati i files prodotti\n",
    "DATASET_DIR = 'datasets/itunes_amazon/'\n",
    "DATASET_NAME ='itunes_amazon'\n",
    "TABLE1_FILE = 'itunes.csv'\n",
    "TABLE2_FILE = 'amazon.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeper.csv2dataset import csv_2_datasetALTERNATE\n",
    "dataset = csv_2_datasetALTERNATE(DATASET_DIR,'matches_itunes_amazon.csv',TABLE1_FILE,TABLE2_FILE, \n",
    "                             [(1,1),(2,2),(3,3)],sim_function=lambda x, y: [1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Bless the Broken Road',\n",
       "  'Rascal Flatts',\n",
       "  'Hannah Montana : The Movie ( Original Motion Picture Soundtrack )'],\n",
       " ['The Movie Theater',\n",
       "  'Brian Tyler',\n",
       "  'The Final Destination ( Original Motion Picture Soundtrack )'],\n",
       " [1, 1],\n",
       " 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8824418982027676\n",
      "0.7768985960673559\n",
      "0.8680003787638844\n",
      "0.8279925162221707\n",
      "0.5316095330711951\n",
      "0.7452413135250994\n",
      "0.8581278705797843\n",
      "0.6183469424008424\n",
      "0.703989469396047\n",
      "0.53813823519705\n",
      "0.8717100546930083\n",
      "0.42640143271122094\n",
      "0.6350528962771202\n",
      "0.6984538909891174\n",
      "0.3682298471593294\n",
      "0.9067647005823629\n",
      "0.36288736930121157\n",
      "0.6351073488299558\n",
      "0.42808634473904467\n",
      "0.876416745784994\n",
      "0.8660254037844386\n",
      "0.8414349769963337\n",
      "0.7858252779857413\n",
      "0.5232559521341829\n",
      "0.917463421851129\n",
      "0.3149290838726275\n",
      "0.4456688116249245\n",
      "0.529150262212918\n",
      "0.7452413135250994\n",
      "0.5218624584427538\n",
      "0.7492686492653553\n",
      "0.7405316311773547\n",
      "0.8404244281272969\n",
      "0.7278602642426106\n",
      "0.6445033866354896\n",
      "0.6004805767690768\n",
      "0.835995575171225\n",
      "0.6669729688499156\n",
      "0.9082682792306083\n",
      "0.5337605126836238\n",
      "0.8770580193070293\n",
      "0.5360562674188974\n",
      "0.7200822998230957\n",
      "0.7115124735378853\n",
      "0.5766967882001443\n",
      "0.8095238095238095\n",
      "0.7926290870042667\n",
      "0.8668451156610704\n",
      "0.5766967882001443\n",
      "0.37062465833055064\n",
      "0.7559289460184544\n",
      "0.6444022325288264\n",
      "0.8718572905786446\n",
      "0.8095238095238095\n",
      "0.8685990362153793\n",
      "0.4296689244236597\n",
      "0.3211764393605811\n",
      "0.42640143271122094\n",
      "0.4120816918460671\n",
      "0.9473684210526314\n",
      "0.5813183589761798\n",
      "0.5144957554275265\n",
      "0.6662522386112826\n",
      "0.35337091141177956\n",
      "0.9558926738785617\n",
      "0.6880624620561868\n",
      "0.7909810627737345\n",
      "0.45425676257949793\n",
      "0.9082682792306083\n",
      "0.8095238095238095\n",
      "0.8500331705159768\n",
      "0.3981380814347516\n",
      "0.42808634473904467\n",
      "0.6246950475544243\n",
      "0.8373041067114761\n",
      "0.8153742483272114\n",
      "0.7368421052631577\n",
      "0.8577503924427757\n",
      "0.6218743352054218\n",
      "0.8500331705159768\n",
      "0.45305152225567213\n",
      "0.6420578831241025\n",
      "0.8178608201095309\n",
      "0.8581278705797843\n",
      "0.7585826061362605\n",
      "0.6546536707079772\n",
      "0.6009252125773316\n",
      "0.4618802153517006\n",
      "0.8667190566019205\n",
      "0.5587442366156625\n",
      "0.6350852961085884\n",
      "0.590168890850654\n",
      "0.7566444492037343\n",
      "0.7319250547113998\n",
      "0.44342202660886976\n",
      "0.42796049251091295\n",
      "0.8680003787638844\n",
      "0.6246950475544243\n",
      "0.5962847939999439\n",
      "0.35337091141177956\n",
      "0.8095238095238095\n",
      "0.7661308776828739\n",
      "0.472455591261534\n",
      "0.9095085938862486\n",
      "0.47249953857489463\n",
      "0.7200822998230957\n",
      "0.7029399400673935\n",
      "0.5520524474738834\n",
      "0.3595732599803958\n",
      "0.5337605126836238\n",
      "0.8095238095238095\n",
      "0.8495103616774144\n",
      "0.4193139346887673\n",
      "0.49304933130229783\n",
      "0.9285714285714286\n",
      "0.3211764393605811\n",
      "0.6172133998483676\n",
      "0.7674213798361965\n",
      "0.6936879756192958\n",
      "0.45425676257949793\n",
      "0.4685212856658182\n",
      "0.9407529036048871\n",
      "0.671222129210352\n",
      "0.6016362990591417\n",
      "0.7368421052631577\n",
      "0.8315218406202999\n",
      "0.45425676257949793\n",
      "0.53813823519705\n",
      "0.9913531291324377\n",
      "0.44543540318737396\n",
      "0.9088932591463857\n",
      "0.4685212856658182\n"
     ]
    }
   ],
   "source": [
    "deeper_train,deeper_test = process_data(DATASET_DIR,'itunes_amazon',ground_truth='matches_itunes_amazon.csv',\n",
    "                         table1=TABLE1_FILE,table2=TABLE2_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Summer Love',\n",
       "  'One Direction',\n",
       "  'Take Me Home',\n",
       "  'Pop , Music , Pop/Rock , Teen Pop , Rock , Dance , World',\n",
       "  '$ 1.29',\n",
       "  '2012 Simco Limited under exclusive licence to Sony Music Entertainment UK Limited'],\n",
       " ['TÌ 1/4 , Entre Mis Cosas',\n",
       "  'Danny Cabuche',\n",
       "  'Y Hoy Me Recuerdas',\n",
       "  'International , Latin Music , Pop , Latin Pop',\n",
       "  '$ 1.29',\n",
       "  '( c ) 2007 Relapse Records , Inc.'],\n",
       " 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeper_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Costruzione indice degli embeddings.....Fatto. 400000 embeddings totali.\n",
      "* Creazione del modello per il calcolo degli embeddings....\n",
      "* Inizializzo il tokenizzatore.....Fatto: 400000 parole totali.\n",
      "* Preparazione della matrice di embedding.....Fatto. Dimensioni matrice embeddings: (400001, 300)\n",
      "\n",
      "°°° EMBEDDING MODEL °°°\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Tupla_A (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Tupla_B (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding_lookup (Embedding)    (None, None, 300)    120000300   Tupla_A[0][0]                    \n",
      "                                                                 Tupla_B[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 120,000,300\n",
      "Trainable params: 0\n",
      "Non-trainable params: 120,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Caricamento strutture dati e modelli ausiliari.\n",
    "embeddings_index = init_embeddings_index(EMBEDDING_FILEPATH)\n",
    "emb_dim = len(embeddings_index['cat']) # :3\n",
    "embeddings_model, tokenizer = init_embeddings_model(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = 0.8\n",
    "emb_dim =300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Addestramento standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# InPut: Percentuale di dati considerata per l'addestramento. \n",
    "# OutPut: DeepER addestrato sul taglio scelto.\n",
    "def get_DeepER(perc):\n",
    "   \n",
    "    sub_data = splitting_dataSet(perc, deeper_train)    \n",
    "    \n",
    "    if LOAD_FROM_DISK_MODEL:\n",
    "        \n",
    "        # Carica da disco.\n",
    "        print(f'Loading DeepER_best_model_{int(perc*100)}_{DATASET_NAME}.h5', end='', flush=True)\n",
    "        deeper_model = load_model(f'DeepER_best_model_{int(perc*100)}_{DATASET_NAME}.h5')\n",
    "        print('  ->  Done')        \n",
    "                \n",
    "    else:\n",
    "        \n",
    "        # Inizializza il modello.\n",
    "        deeper_model = init_DeepER_model(emb_dim)\n",
    "        deeper_model.summary()\n",
    "        # Avvio addestramento.\n",
    "        deeper_model = train_model_ER(sub_data, \n",
    "                                      deeper_model, \n",
    "                                      embeddings_model, \n",
    "                                      tokenizer, \n",
    "                                      pretraining=False,\n",
    "                                      metric='val_accuracy',\n",
    "                                      end=f'_{int(perc*100)}_{DATASET_NAME}')\n",
    "        \n",
    "    return deeper_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "°°° DeepER Model °°°\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Embeddings_seq_a (InputLayer)   (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embeddings_seq_b (InputLayer)   (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Composition (Bidirectional)     (None, 300)          541200      Embeddings_seq_a[0][0]           \n",
      "                                                                 Embeddings_seq_b[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Similarity (Lambda)             (None, 300)          0           Composition[0][0]                \n",
      "                                                                 Composition[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Dense1 (Dense)                  (None, 300)          90300       Similarity[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Dense2 (Dense)                  (None, 300)          90300       Dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Classification (Dense)          (None, 2)            602         Dense2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 722,402\n",
      "Trainable params: 722,402\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "* Preparazione input......Fatto. 105 tuple totali, esempio label: 1 -> [0. 1.], Table1 shape: (105, 61), Table2 shape: (105, 49)\n",
      "Batch size: 3\n",
      "Train on 84 samples, validate on 21 samples\n",
      "Epoch 1/64\n",
      "84/84 [==============================] - 7s 78ms/step - loss: 0.6829 - accuracy: 0.6071 - val_loss: 0.7119 - val_accuracy: 0.4286\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.42857, saving model to models\\DeepER_best_model_50_itunes_amazon.h5\n",
      "Epoch 2/64\n",
      "84/84 [==============================] - 5s 61ms/step - loss: 0.5744 - accuracy: 0.7619 - val_loss: 0.5104 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.42857 to 0.71429, saving model to models\\DeepER_best_model_50_itunes_amazon.h5\n",
      "Epoch 3/64\n",
      "84/84 [==============================] - 5s 65ms/step - loss: 0.2825 - accuracy: 0.9405 - val_loss: 0.6078 - val_accuracy: 0.7619\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.71429 to 0.76190, saving model to models\\DeepER_best_model_50_itunes_amazon.h5\n",
      "Epoch 4/64\n",
      "84/84 [==============================] - 5s 57ms/step - loss: 0.0610 - accuracy: 0.9881 - val_loss: 0.3995 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.76190 to 0.85714, saving model to models\\DeepER_best_model_50_itunes_amazon.h5\n",
      "Epoch 5/64\n",
      "84/84 [==============================] - 5s 60ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.2468 - val_accuracy: 0.9048\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.85714 to 0.90476, saving model to models\\DeepER_best_model_50_itunes_amazon.h5\n",
      "Epoch 6/64\n",
      "84/84 [==============================] - 5s 57ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 1.7514 - val_accuracy: 0.6190\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.90476\n",
      "Epoch 7/64\n",
      "84/84 [==============================] - 5s 58ms/step - loss: 0.0438 - accuracy: 0.9762 - val_loss: 0.3256 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.90476\n",
      "Epoch 8/64\n",
      "84/84 [==============================] - 5s 58ms/step - loss: 0.0356 - accuracy: 0.9881 - val_loss: 0.6035 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.90476\n",
      "Epoch 9/64\n",
      "84/84 [==============================] - 5s 58ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.5640 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.90476\n",
      "Epoch 10/64\n",
      "84/84 [==============================] - 5s 60ms/step - loss: 8.3116e-04 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.90476\n",
      "Epoch 11/64\n",
      "84/84 [==============================] - 6s 70ms/step - loss: 5.0099e-04 - accuracy: 1.0000 - val_loss: 0.5892 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.90476\n",
      "Epoch 12/64\n",
      "84/84 [==============================] - 6s 76ms/step - loss: 3.3597e-04 - accuracy: 1.0000 - val_loss: 0.6005 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.90476\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Avvio addestramenti o carica da disco.\n",
    "deeper_model_100 = get_DeepER(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcolo F-Measure dopo addestramento standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Misurazione dell'f-measure sullo stesso test set con i diversi modelli.\n",
    "f1_score= model_statistics(deeper_test, deeper_model_100, embeddings_model, tokenizer)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizzazione F-Measure: primi risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Attiva modalità notebook per mostrare i grafici correttamente.\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "splits = ['100% split', '75% split', '50% split', '25% split', '10% split', '5% split']\n",
    "total_tup = len(deeper_train)\n",
    "tuplecount = [total_tup, \n",
    "              int(total_tup*0.75), \n",
    "              int(total_tup*0.5), \n",
    "              int(total_tup*0.25), \n",
    "              int(total_tup*0.1), \n",
    "              int(total_tup*0.05)]\n",
    "\n",
    "# Aggiungi descrizione al numero\n",
    "tuplecount = list(map(lambda x: f'{x} coppie di tuple', tuplecount))\n",
    "\n",
    "fig = go.Figure(data=[go.Bar(name='DeepER', x=splits, y=fm_model_standard, hovertext=tuplecount)])\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "# Plotta il grafico e salvalo come features_standard.html (verrà integrato nell'html).\n",
    "pyo.iplot(fig, filename='fmeasures-standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Al passaggio del mouse il grafico mostra:\n",
    "- Il numero di coppie di tuple utilizzate per l'addestramento; \n",
    "- La percentuale di split (Quantità di tuple utilizzate per addestrare il modello);\n",
    "- Il valore di F-Measure (media armonica tra precision e recall);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalute model with shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "walmart_amazon_model = load_model('models/DeepER_best_model_100_walmart_amazon.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepER import data2Inputs\n",
    "table1, table2, labels = data2Inputs(deeper_train, tokenizer, categorical=False)\n",
    "embeddings = embeddings_model.predict([table1,table2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_test,table2_test,labels = data2Inputs(deeper_test,tokenizer,categorical=False)\n",
    "testembeddings = embeddings_model.predict([table1_test,table2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "def get_layer_output(model,layer_name,data):\n",
    "    intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n",
    "    intermediate_output = intermediate_layer_model.predict(data)\n",
    "    return intermediate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def get_layer_output_grad(model, inputs, outputs, layer_name):\n",
    "    \"\"\" Gets gradient a layer output for given inputs and outputs\"\"\"\n",
    "    grads = model.optimizer.get_gradients(model.total_loss, model.get_layer(layer_name).output)\n",
    "    symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
    "    f = K.function(symb_inputs, grads)\n",
    "    x, y, sample_weight = model._standardize_user_data(inputs, outputs)\n",
    "    output_grad = f(x + y + sample_weight)\n",
    "    return output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = walmart_amazon_model.predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = get_layer_output_grad(walmart_amazon_model,embeddings,predictions,'Dense1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "idx = 4  # index of desired layer\n",
    "input_shape = walmart_amazon_model.layers[idx].get_input_shape_at(0) # get the input shape of desired layer\n",
    "layer_input = Input(shape=(300,)) # a new input tensor to be able to feed the desired layer\n",
    "# create the new nodes for each layer in the path\n",
    "x = layer_input\n",
    "for layer in walmart_amazon_model.layers[idx:]:\n",
    "    x = layer(x)\n",
    "\n",
    "# create the model\n",
    "new_model = Model(layer_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.predict(similarity_output)[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_amazon_model.predict(embeddings)[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(fodors_zagats_model,embeddings)\n",
    "shap_values = explainer.shap_values(testembeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
